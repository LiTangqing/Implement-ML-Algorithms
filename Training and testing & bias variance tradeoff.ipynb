{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Versus Testing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Error Measures**\n",
    "- User-specified e(h(x),f(x))\n",
    "- In-sample $E_{in}(h) = \\frac{1}{N}\\sum_{n=1}^Ne(h(x_n), f(x_n))$\n",
    "- Out-of-sample $E_{out}(h)=E_x[e(h(x),f(x))]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Noisy Targets.**\n",
    "The relationship that we are trying to learn may not be a deterministic function of x.\n",
    "![title](img/noisy_target.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $(x_1, y_1), ..., (x_n,y_n)$ generated by \n",
    "$$P(x,y) = P(x)p(y|x)$$\n",
    "- $E_{out}(h)$ is now $E_{x,y}[e(h(x),y)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias Variance Tradeoff\n",
    "approximation-generalization tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samll $E_{out}$: good approximation of f out of sample\n",
    "\n",
    "More complex H -> better chance of approximating f\n",
    "\n",
    "Less complex H -> better chance of generalizing out of sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias Variance analysis\n",
    "Decomposing $E{out}$ into \n",
    "1. How well H can approximate f\n",
    "2. how well can we zoom in on to a good $h \\in H$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "out of sample error for a specific dataset:\n",
    "\n",
    "$E_{out}(g^D) = E_x[(g^D(x)-f(x))^2] $\n",
    "\n",
    "Now we try to find the expectation of the out of sample error given a specific size of dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "E_D[E_{out}(g^D)] & = E_D[ E_x[(g^D(x)-f(x))^2]]\\\\\n",
    "& = E_x[ E_D[(g^D(x)-f(x))^2]]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we focus on $ E_D[(g^D(x)-f(x))^2]$. **This quantity tells how far your hopo learned from the (particular)dataset differs from the ultimate target.**\n",
    "\n",
    "\n",
    "\n",
    "To evaluate this, we define the 'average' hypo $\\bar g(x)$\n",
    "\n",
    "$\\bar g(x) = E_D[g^D(x)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "E_D[(g^D(x)-f(x))^2] & = E_D[(g^D(x)-\\bar g(x) + \\bar g(x)-f(x))^2] \\\\\n",
    "& = E_D[(g^D(x)-\\bar g(x))^2 + (\\bar g(x)-f(x))^2) + 2(g^D(x)-\\bar g(x))(\\bar g(x)-f(x))] \\\\\n",
    "& = E_D[(g^D(x)-\\bar g(x))^2] + (\\bar g(x)-f(x))^2\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variance.** $E_D[(g^D(x)-\\bar g(x))^2]$ measures how far your hopo learnt from a particular dataset depart from the best hypo you can get from your hopothesis set.\n",
    "\n",
    "**Bias.** $(\\bar g(x)-f(x))^2$ measures how far the best hypo from the hypothesis set is from the target function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, \n",
    "\\begin{align}\n",
    "E_D[E_{out}(g^D)] & = E_x[ E_D[(g^D(x)-f(x))^2]]\\\\\n",
    "&= E_x[bias(x)+var(x)] \\\\\n",
    "&=bias+var\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rule of Thumb!\n",
    "\n",
    "Always match the model complexity to the data resources, not to the target complexity!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![learning curve](img/learning_curve.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "1. Complex models approximate better than simple models(the expected error is lower). \n",
    "2. Generalisation ability can be gauged by the discrapency of E_in and E_out. Simple models have less generalisation error (more powerful in generalisation)\n",
    "\n",
    "So which one is better? - match the model by the resource that you have. e.g.\n",
    "1. Depends on number of examples (N). if N is small, you cannot afford a complex model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![vc analysis vs bias variance tradeoff](img/vc&bv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting is different from bad generalisation.\n",
    "Overfitting can happen in the same model.\n",
    "\n",
    "Overfitting: when $E_{in}$ goes down and $E_{out}$ goes up. -> fitting the noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even when the target function has no noise, overfitting can still occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impact of noise level and target complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![overfitting](img/overfitting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: red -> overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "first firgure suggests noise stochastic \n",
    "error caused by having too complex mode: deterministic noise\n",
    "\n",
    "Observations:\n",
    "1. number of data points goes up, overfitting goes down\n",
    "2. stochastic noise/deterministic noise goes up -> overfitting goes up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition of deterministic noise**: the part of f that H cannot capture. \n",
    "\n",
    "Its main differences with stochastic noise is that \n",
    "\n",
    "1. deterministic noise depends on H. \n",
    "2. its fixed for a given x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**However**, for finite N, H tried to fit the noise (including the deterministic noise)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise and bias-variance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "E_D[(g^D(x)-y)^2] & = E_D[(g^D(x)-\\bar g(x) + \\bar g(x)-f(x) - \\epsilon(x))^2] \\\\\n",
    "& = E_D[(g^D(x)-\\bar g(x))^2 + (\\bar g(x)-f(x))^2 + \\epsilon(x)^2 + crossterm] \\\\\n",
    "& = E_D[(g^D(x)-\\bar g(x))^2] + (\\bar g(x)-f(x))^2 + \\sigma ^2\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bias = deterministic noise; fixed given a hypo set!\n",
    "\n",
    "$\\sigma$: stochastic noise\n",
    "\n",
    "The model will tries to capture both and thats why there is a variance term, because the model cannot tell signal from noise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to deal with overfitting - regularization & validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$E_{out}(h) = E_{in}{h} +$ overfit penalty\n",
    "\n",
    "regularization estimate the overfit penalty, while validation estimate the out-of-sample error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on a validation set $(x_1, y_1), .., (x_k,y_k)$. the error is $E_{val}(h) = \\frac{1}{K}\\sum_1^K e(h(e_k),y_k)$\n",
    "\n",
    "So how reliable is this estimate?\n",
    "\n",
    "$E[E_{val}(h)] = E_{out}(h)$\n",
    "\n",
    "$var[E_{val}(h)] = \\frac{\\sigma^2}{K}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implies that, when K is small, we will have a bad estimate (variance too high). \n",
    "\n",
    "If K is very large, we are having a reliable estimate, but since we now have less data in the train set, we end up having a reliable estimate of a bad model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### difference between test set and validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test set is unbiased; validation set has optimistic bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
