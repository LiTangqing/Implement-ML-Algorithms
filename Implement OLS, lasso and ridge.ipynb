{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math behind the model (matrix approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Matrix formulation\n",
    "The matrix formulation of the model is \n",
    "\n",
    "$$Y = X\\theta+ \\epsilon$$\n",
    "\n",
    "where $Y$ is an $(n \\times 1)$ vector of response; \n",
    "\n",
    "$X$ is an $(n \\times 2)$ matrix; \n",
    "\n",
    "$\\theta$ is $(2 \\times 1)$ vector of unknown parameters and \n",
    "\n",
    "$\\epsilon$ is an $(n \\times 1)$ vectors of error terms; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumptions about the random error: $\\epsilon$ follows $N_n(0, \\sigma^2I)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$X^TY = X^TX\\hat{\\theta}$$\n",
    "This can be obtained by least square method.\n",
    "\n",
    "It follows that so long as $X^TX$ is **invertible** i.e. its determinant is non-zero, the unique solution to the normal equation is given by:\n",
    "\n",
    "\n",
    "$$\\hat{\\theta} = (X^TX)^{-1}X^TY$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How is normal equation derived? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Least square approach\n",
    "\n",
    "The lost function of OLS: $$J(\\theta) = \\frac{1}{m}(y-X\\theta)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The esimated coefficients are found by minimising the loss function. To minimise the loss function, we can inspect the derivative of it first. (to reduce typing work ill ignore the 1/m for now)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\triangledown J(\\theta) & = \\triangledown <y-X\\theta, y-X\\theta> \\\\\n",
    " & = \\triangledown_\\theta(<y, y> + <X\\theta, X\\theta> -2<y, X\\theta>)\\\\\n",
    " & = \\triangledown_\\theta(<X\\theta, X\\theta>) -2\\triangledown_\\theta<X^Ty, \\theta>\\\\\n",
    " & = \\triangledown_\\theta(X\\theta)2X\\theta-2X^Ty\\\\\n",
    " & = 2X^TX\\theta-2X^Ty\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second order derivative (Hessian):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "Hess(J(\\theta)) & = \\triangledown_\\theta(2X^TX\\theta-2X^Ty)\\\\\n",
    "& = 2 (X^TX)^T\\\\\n",
    "& = 2X^TX\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what is this $X^TX$? \n",
    "\n",
    "Assuming that X has independent columns i.e. X has full rank, $X^TX$ is PSD (positive semi definite).\n",
    "\n",
    "How to prove that$ X^TX$ is PSD -\n",
    "\n",
    "First, $X^TX$ is symmetric; To prove that $X^TX$ is PSD, we only need to prove that $<v, X^TXv> \\geqslant 0$, for any v. And this is actually true because \n",
    "$$<v, X^TXv> = <Xv, Xv> = ||Xv||^2 \\geqslant 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is proved that $X^TX$ is PDS -> $J(\\theta)$ is **convex** and has a global minimum. So to get the desired $\\theta$ that minimise the cost function, we can simply make the first order derivative to 0. That is,\n",
    "$$2X^TX\\theta-2X^Ty=0$$ \n",
    "which gives $$\\hat\\theta= (X^TX)^{-1}X^Ty$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 How to estimate θ? - Gradient Descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to choose θ so as to minimize J(θ) and we could do this using **gradient descent**. \n",
    "\n",
    "The general idea is to start with some initial guesses and keep updating the parameter to make J(θ) smaller, until converge(hopefully). \n",
    "\n",
    "How we perform the update: \n",
    "\n",
    "$$θj :=θj −α  \\triangledown J(θ)$$\n",
    "\n",
    "where $\\alpha$ is the learning rate; \n",
    "\n",
    "And we can see that in each update, we takes a step in the direction of steepest **decrease** (opposite of increasing direction) of J."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch GD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic GD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
